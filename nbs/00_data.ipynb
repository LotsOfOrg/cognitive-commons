{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Management\n",
    "> Core functionality for data ingestion, hashing, and quality checking.\n",
    "\n",
    "This module handles:\n",
    "- Data ingestion and storage\n",
    "- Content hashing and ID generation\n",
    "- Basic quality checks\n",
    "- Content provenance tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data\n",
    "#| export\n",
    "from fastcore.basics import *\n",
    "from fastcore.test import *\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime,timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking inspiration from FineWeb\n",
    "FineWeb use a multi-stage filtering approach that we could look to inspire a `DataQualityChecker` class.\n",
    "\n",
    "The key insights we're implementing from FineWeb are:\n",
    "1. Multi-stage quality filtering\n",
    "2. Focus on line-level quality metrics\n",
    "3. Tracking content reuse and impact\n",
    "4. Weighted scoring based on historical performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataQualityChecker:\n",
    "    \"\"\"Quality checking pipeline inspired by FineWeb's approach\"\"\"\n",
    "    def __init__(self, min_quality_score: float = 0.12):\n",
    "        self.min_quality_score = min_quality_score\n",
    "    \n",
    "    def check_basic_quality(self, content: str) -> Dict[str, float]:\n",
    "        \"\"\"Basic quality checks similar to FineWeb's base filtering\"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        # Check fraction of lines ending with punctuation\n",
    "        lines = content.split('\\n')\n",
    "        punct_lines = sum(1 for l in lines if l.strip() and l.strip()[-1] in '.!?')\n",
    "        scores['punctuation_ratio'] = punct_lines / len(lines) if lines else 0\n",
    "        \n",
    "        # Check for duplicate lines (similar to FineWeb's line deduplication)\n",
    "        unique_lines = set(lines)\n",
    "        scores['unique_lines_ratio'] = len(unique_lines) / len(lines) if lines else 0\n",
    "        \n",
    "        # Check for short lines (FineWeb removes docs with too many short lines)\n",
    "        short_lines = sum(1 for l in lines if len(l.strip()) < 30)\n",
    "        scores['short_lines_ratio'] = short_lines / len(lines) if lines else 1\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def calculate_quality_score(self, content: str) -> float:\n",
    "        \"\"\"Calculate overall quality score\"\"\"\n",
    "        scores = self.check_basic_quality(content)\n",
    "        \n",
    "        # Weights could be tuned based on empirical results\n",
    "        quality_score = (\n",
    "            0.4 * scores['punctuation_ratio'] +\n",
    "            0.4 * scores['unique_lines_ratio'] +\n",
    "            0.2 * (1 - scores['short_lines_ratio'])\n",
    "        )\n",
    "        \n",
    "        return quality_score\n",
    "    \n",
    "    def is_acceptable(self, content: str) -> Tuple[bool, Dict[str, float]]:\n",
    "        \"\"\"Check if content meets minimum quality standards\"\"\"\n",
    "        scores = self.check_basic_quality(content)\n",
    "        quality_score = self.calculate_quality_score(content)\n",
    "        return quality_score >= self.min_quality_score, {\n",
    "            'quality_score': quality_score,\n",
    "            **scores\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can implement a citation-like systme similar to how FineWeb tracks data reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ContentProvenance:\n",
    "    \"\"\"Tracks content reuse and impact across training runs\"\"\"\n",
    "    def __init__(self):\n",
    "        self.citation_graph = {}  # content_id -> list of derived_content_ids\n",
    "        self.usage_history = {}   # content_id -> list of model_version_ids\n",
    "        self.quality_history = {} # content_id -> list of quality scores\n",
    "        \n",
    "    def record_usage(self, content_id: str, model_version: str, \n",
    "                    quality_impact: float):\n",
    "        \"\"\"Record when content is used in training\"\"\"\n",
    "        if content_id not in self.usage_history:\n",
    "            self.usage_history[content_id] = []\n",
    "        if content_id not in self.quality_history:\n",
    "            self.quality_history[content_id] = []\n",
    "            \n",
    "        self.usage_history[content_id].append(model_version)\n",
    "        self.quality_history[content_id].append(quality_impact)\n",
    "    \n",
    "    def calculate_impact_score(self, content_id: str) -> float:\n",
    "        \"\"\"Calculate content impact based on usage and quality history\"\"\"\n",
    "        if content_id not in self.quality_history:\n",
    "            return 0.0\n",
    "            \n",
    "        # Average quality impact weighted by recency\n",
    "        weights = np.exp(np.arange(len(self.quality_history[content_id])))\n",
    "        weighted_avg = np.average(\n",
    "            self.quality_history[content_id], \n",
    "            weights=weights\n",
    "        )\n",
    "        \n",
    "        # Boost score based on number of uses\n",
    "        usage_multiplier = 1 + np.log1p(len(self.usage_history[content_id]))\n",
    "        \n",
    "        return weighted_avg * usage_multiplier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| test\n",
    "def test_quality_checker():\n",
    "    checker = DataQualityChecker(min_quality_score=0.5)\n",
    "    \n",
    "    # Test high quality content\n",
    "    good_content = \"This is a well-formatted paragraph.\\nIt has proper punctuation.\\nAnd good length lines.\"\n",
    "    is_good, scores = checker.is_acceptable(good_content)\n",
    "    test_eq(is_good, True)\n",
    "    test_gt(scores['quality_score'], 0.5)\n",
    "    \n",
    "    # Test low quality content\n",
    "    bad_content = \"short\\nshort\\nshort\\nno punct\\nrepeated\\nrepeated\"\n",
    "    is_bad, scores = checker.is_acceptable(bad_content)\n",
    "    test_eq(is_bad, False)\n",
    "    test_lt(scores['quality_score'], 0.5)\n",
    "\n",
    "def test_provenance():\n",
    "    prov = ContentProvenance()\n",
    "    \n",
    "    # Test recording usage\n",
    "    prov.record_usage(\"content_1\", \"model_v1\", 0.8)\n",
    "    prov.record_usage(\"content_1\", \"model_v2\", 0.9)\n",
    "    \n",
    "    # Test impact calculation\n",
    "    impact = prov.calculate_impact_score(\"content_1\")\n",
    "    test_gt(impact, 0.8)  # Should be > 0.8 due to weighted average\n",
    "    \n",
    "    # Test nonexistent content\n",
    "    test_eq(prov.calculate_impact_score(\"nonexistent\"), 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ContentStore:\n",
    "    \"\"\"Manages storage and retrieval of training data content with quality checking and provenance tracking\"\"\"\n",
    "    def __init__(self, store_path: Path, min_quality_score: float = 0.12):\n",
    "        self.store_path = Path(store_path)\n",
    "        self.store_path.mkdir(exist_ok=True)\n",
    "        self.index_file = self.store_path/'index.json'\n",
    "        self.quality_checker = DataQualityChecker(min_quality_score=min_quality_score)\n",
    "        self.provenance = ContentProvenance()\n",
    "        self._load_index()\n",
    "    \n",
    "    def _load_index(self):\n",
    "        \"\"\"Load or initialize the content index\"\"\"\n",
    "        if self.index_file.exists():\n",
    "            self.index = json.loads(self.index_file.read_text())\n",
    "        else:\n",
    "            self.index = {}\n",
    "            self._save_index()\n",
    "    \n",
    "    def _save_index(self):\n",
    "        \"\"\"Save the current index to disk\"\"\"\n",
    "        self.index_file.write_text(json.dumps(self.index, indent=2))\n",
    "    \n",
    "    def add_content(self, content: str, contributor_id: str) -> Tuple[Optional[str], Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Add new content to the store if it passes quality checks\n",
    "        Returns: (content_id, quality_scores) or (None, quality_scores) if rejected\n",
    "        \"\"\"\n",
    "        # Check quality first\n",
    "        is_acceptable, quality_scores = self.quality_checker.is_acceptable(content)\n",
    "        if not is_acceptable:\n",
    "            return None, quality_scores\n",
    "        \n",
    "        content_id = hashlib.sha256(content.encode()).hexdigest()\n",
    "        if content_id in self.index:\n",
    "            return content_id, quality_scores\n",
    "        \n",
    "        timestamp = datetime.now(timezone.utc).isoformat()\n",
    "        self.index[content_id] = {\n",
    "            'contributor_id': contributor_id,\n",
    "            'timestamp': timestamp,\n",
    "            'size': len(content),\n",
    "            'quality_scores': quality_scores\n",
    "        }\n",
    "        \n",
    "        (self.store_path/content_id).write_text(content)\n",
    "        self._save_index()\n",
    "        return content_id, quality_scores\n",
    "    \n",
    "    def record_usage(self, content_id: str, model_version: str, quality_impact: float):\n",
    "        \"\"\"Record content usage in model training\"\"\"\n",
    "        if content_id not in self.index:\n",
    "            raise ValueError(f\"Content ID {content_id} not found\")\n",
    "        \n",
    "        self.provenance.record_usage(content_id, model_version, quality_impact)\n",
    "        \n",
    "        # Update index with latest impact score\n",
    "        self.index[content_id]['impact_score'] = self.provenance.calculate_impact_score(content_id)\n",
    "        self._save_index()\n",
    "    \n",
    "    def get_content(self, content_id: str) -> Optional[str]:\n",
    "        \"\"\"Retrieve content by ID\"\"\"\n",
    "        if content_id not in self.index:\n",
    "            return None\n",
    "        return (self.store_path/content_id).read_text()\n",
    "    \n",
    "    def get_metadata(self, content_id: str) -> Optional[Dict]:\n",
    "        \"\"\"Get all metadata for a content item\"\"\"\n",
    "        return self.index.get(content_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| test\n",
    "@patch\n",
    "def cleanup(self:ContentStore):\n",
    "    \"Remove all files in the store\"\n",
    "    if self.store_path.exists():\n",
    "        for f in self.store_path.glob('*'):\n",
    "            f.unlink()\n",
    "        self.store_path.rmdir()\n",
    "\n",
    "#| test\n",
    "def test_enhanced_content_store():\n",
    "    # Setup\n",
    "    store = ContentStore(Path('test_store'), min_quality_score=0.5)\n",
    "    \n",
    "    # Test adding high-quality content\n",
    "    good_content = \"This is a well-formatted paragraph.\\nIt has proper punctuation.\\nAnd good length lines.\"\n",
    "    content_id, scores = store.add_content(good_content, \"test_user_1\")\n",
    "    \n",
    "    # Verify content was accepted and stored\n",
    "    test_not_none(content_id)\n",
    "    test_gt(scores['quality_score'], 0.5)\n",
    "    \n",
    "    # Test adding low-quality content\n",
    "    bad_content = \"short\\nshort\\nshort\\nno punct\\nrepeated\\nrepeated\"\n",
    "    rejected_id, bad_scores = store.add_content(bad_content, \"test_user_2\")\n",
    "    \n",
    "    # Verify content was rejected\n",
    "    test_none(rejected_id)\n",
    "    test_lt(bad_scores['quality_score'], 0.5)\n",
    "    \n",
    "    # Test usage recording and impact scoring\n",
    "    store.record_usage(content_id, \"model_v1\", 0.8)\n",
    "    store.record_usage(content_id, \"model_v2\", 0.9)\n",
    "    \n",
    "    # Check metadata includes impact score\n",
    "    metadata = store.get_metadata(content_id)\n",
    "    test_not_none(metadata.get('impact_score'))\n",
    "    test_gt(metadata['impact_score'], 0.8)\n",
    "    \n",
    "    # Cleanup\n",
    "    store.cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content quality scores: {'quality_score': 0.68, 'punctuation_ratio': 0.6, 'unique_lines_ratio': 0.8, 'short_lines_ratio': 0.4}\n",
      "Content ID: 59dc718b... (from alice)\n",
      "\n",
      "Content metadata:\n",
      "{\n",
      "  \"contributor_id\": \"alice\",\n",
      "  \"timestamp\": \"2025-02-16T12:57:55.960288+00:00\",\n",
      "  \"size\": 151,\n",
      "  \"quality_scores\": {\n",
      "    \"quality_score\": 0.68,\n",
      "    \"punctuation_ratio\": 0.6,\n",
      "    \"unique_lines_ratio\": 0.8,\n",
      "    \"short_lines_ratio\": 0.4\n",
      "  },\n",
      "  \"impact_score\": 1.8323106826194044\n",
      "}\n",
      "\n",
      "Rejected content scores: {'quality_score': 0.30000000000000004, 'punctuation_ratio': 0.0, 'unique_lines_ratio': 0.75, 'short_lines_ratio': 1.0}\n"
     ]
    }
   ],
   "source": [
    "#| example\n",
    "store = ContentStore(Path('example_store'), min_quality_score=0.5)\n",
    "\n",
    "# Try adding high-quality content\n",
    "good_content = \"\"\"\n",
    "This is a well-formatted document with proper sentences.\n",
    "It contains multiple paragraphs with good structure.\n",
    "Each line ends with proper punctuation.\n",
    "\"\"\"\n",
    "\n",
    "content_id, scores = store.add_content(good_content, \"alice\")\n",
    "print(f\"Content quality scores: {scores}\")\n",
    "print(f\"Content ID: {content_id[:8]}... (from alice)\")\n",
    "\n",
    "# Record some usage history\n",
    "store.record_usage(content_id, \"model_v1\", 0.8)\n",
    "store.record_usage(content_id, \"model_v2\", 0.9)\n",
    "\n",
    "# Check the metadata\n",
    "metadata = store.get_metadata(content_id)\n",
    "print(f\"\\nContent metadata:\")\n",
    "print(json.dumps(metadata, indent=2))\n",
    "\n",
    "# Try adding low-quality content\n",
    "bad_content = \"short\\nno punct\\nrepeated\\nrepeated\"\n",
    "rejected_id, bad_scores = store.add_content(bad_content, \"bob\")\n",
    "print(f\"\\nRejected content scores: {bad_scores}\")\n",
    "\n",
    "# Cleanup\n",
    "store.cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
