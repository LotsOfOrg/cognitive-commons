# Data Management


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

## Taking inspiration from FineWeb

FineWeb use a multi-stage filtering approach that we could look to
inspire a
[`DataQualityChecker`](https://LotsOfOrg.github.io/cognitive-commons/data.html#dataqualitychecker)
class.

The key insights we’re implementing from FineWeb are: 1. Multi-stage
quality filtering 2. Focus on line-level quality metrics 3. Tracking
content reuse and impact 4. Weighted scoring based on historical
performance

------------------------------------------------------------------------

<a
href="https://github.com/LotsOfOrg/cognitive-commons/blob/main/cognitive_commons/data.py#L20"
target="_blank" style="float:right; font-size:smaller">source</a>

### DataQualityChecker

>  DataQualityChecker (min_quality_score:float=0.12)

*Quality checking pipeline inspired by FineWeb’s approach*

Similarly we can implement a citation-like systme similar to how FineWeb
tracks data reuse

------------------------------------------------------------------------

<a
href="https://github.com/LotsOfOrg/cognitive-commons/blob/main/cognitive_commons/data.py#L68"
target="_blank" style="float:right; font-size:smaller">source</a>

### ContentProvenance

>  ContentProvenance ()

*Tracks content reuse and impact across training runs*

``` python
def test_quality_checker():
    checker = DataQualityChecker(min_quality_score=0.5)
    
    # Test high quality content
    good_content = "This is a well-formatted paragraph.\nIt has proper punctuation.\nAnd good length lines."
    is_good, scores = checker.is_acceptable(good_content)
    test_eq(is_good, True)
    test_gt(scores['quality_score'], 0.5)
    
    # Test low quality content
    bad_content = "short\nshort\nshort\nno punct\nrepeated\nrepeated"
    is_bad, scores = checker.is_acceptable(bad_content)
    test_eq(is_bad, False)
    test_lt(scores['quality_score'], 0.5)

def test_provenance():
    prov = ContentProvenance()
    
    # Test recording usage
    prov.record_usage("content_1", "model_v1", 0.8)
    prov.record_usage("content_1", "model_v2", 0.9)
    
    # Test impact calculation
    impact = prov.calculate_impact_score("content_1")
    test_gt(impact, 0.8)  # Should be > 0.8 due to weighted average
    
    # Test nonexistent content
    test_eq(prov.calculate_impact_score("nonexistent"), 0.0)
```

------------------------------------------------------------------------

<a
href="https://github.com/LotsOfOrg/cognitive-commons/blob/main/cognitive_commons/data.py#L105"
target="_blank" style="float:right; font-size:smaller">source</a>

### ContentStore

>  ContentStore (store_path:pathlib.Path, min_quality_score:float=0.12)

*Manages storage and retrieval of training data content with quality
checking and provenance tracking*

``` python
@patch
def cleanup(self:ContentStore):
    "Remove all files in the store"
    if self.store_path.exists():
        for f in self.store_path.glob('*'):
            f.unlink()
        self.store_path.rmdir()

#| test
def test_enhanced_content_store():
    # Setup
    store = ContentStore(Path('test_store'), min_quality_score=0.5)
    
    # Test adding high-quality content
    good_content = "This is a well-formatted paragraph.\nIt has proper punctuation.\nAnd good length lines."
    content_id, scores = store.add_content(good_content, "test_user_1")
    
    # Verify content was accepted and stored
    test_not_none(content_id)
    test_gt(scores['quality_score'], 0.5)
    
    # Test adding low-quality content
    bad_content = "short\nshort\nshort\nno punct\nrepeated\nrepeated"
    rejected_id, bad_scores = store.add_content(bad_content, "test_user_2")
    
    # Verify content was rejected
    test_none(rejected_id)
    test_lt(bad_scores['quality_score'], 0.5)
    
    # Test usage recording and impact scoring
    store.record_usage(content_id, "model_v1", 0.8)
    store.record_usage(content_id, "model_v2", 0.9)
    
    # Check metadata includes impact score
    metadata = store.get_metadata(content_id)
    test_not_none(metadata.get('impact_score'))
    test_gt(metadata['impact_score'], 0.8)
    
    # Cleanup
    store.cleanup()
```

``` python
store = ContentStore(Path('example_store'), min_quality_score=0.5)

# Try adding high-quality content
good_content = """
This is a well-formatted document with proper sentences.
It contains multiple paragraphs with good structure.
Each line ends with proper punctuation.
"""

content_id, scores = store.add_content(good_content, "alice")
print(f"Content quality scores: {scores}")
print(f"Content ID: {content_id[:8]}... (from alice)")

# Record some usage history
store.record_usage(content_id, "model_v1", 0.8)
store.record_usage(content_id, "model_v2", 0.9)

# Check the metadata
metadata = store.get_metadata(content_id)
print(f"\nContent metadata:")
print(json.dumps(metadata, indent=2))

# Try adding low-quality content
bad_content = "short\nno punct\nrepeated\nrepeated"
rejected_id, bad_scores = store.add_content(bad_content, "bob")
print(f"\nRejected content scores: {bad_scores}")

# Cleanup
store.cleanup()
```

    Content quality scores: {'quality_score': 0.68, 'punctuation_ratio': 0.6, 'unique_lines_ratio': 0.8, 'short_lines_ratio': 0.4}
    Content ID: 59dc718b... (from alice)

    Content metadata:
    {
      "contributor_id": "alice",
      "timestamp": "2025-02-16T12:57:55.960288+00:00",
      "size": 151,
      "quality_scores": {
        "quality_score": 0.68,
        "punctuation_ratio": 0.6,
        "unique_lines_ratio": 0.8,
        "short_lines_ratio": 0.4
      },
      "impact_score": 1.8323106826194044
    }

    Rejected content scores: {'quality_score': 0.30000000000000004, 'punctuation_ratio': 0.0, 'unique_lines_ratio': 0.75, 'short_lines_ratio': 1.0}
